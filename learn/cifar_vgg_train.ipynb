{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(keras.models.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\"\n",
    "        :param input_shape: [32, 32, 3]\n",
    "        \"\"\"\n",
    "        super(VGG16, self).__init__()\n",
    "        weight_decay = 0.000\n",
    "        self.num_classes = 10\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Conv2D(64, (3, 3), padding='same', input_shape=input_shape, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(64, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(512,kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "        model.add(keras.layers.Dense(self.num_classes))\n",
    "        # model.add(layers.Activation('softmax'))\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_features_and_labels(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "def make_dataset(data,label):\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((data, label))\n",
    "    dataset = dataset.map(prepare_features_and_labels)\n",
    "    dataset = dataset.shuffle(10000).batch(100)\n",
    "    return dataset\n",
    "\n",
    "dataset_train=make_dataset(x_train,y_train)\n",
    "dataset_test=make_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16([32, 32, 3])\n",
    "# must specify from_logits=True!\n",
    "criteon = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metric = keras.metrics.CategoricalAccuracy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 step= 0 loss: 2.192420482635498 acc: 0.20333333 max_grad: 6.5817246\n",
      "epoch= 0 step= 40 loss: 2.043679714202881 acc: 0.1985 max_grad: 2.9994366\n",
      "epoch= 0 step= 80 loss: 2.0044972896575928 acc: 0.217 max_grad: 7.845404\n",
      "epoch= 0 step= 120 loss: 1.944870114326477 acc: 0.22875 max_grad: 2.7888246\n",
      "epoch= 0 step= 160 loss: 1.908591628074646 acc: 0.25375 max_grad: 15.0\n",
      "epoch= 0 step= 200 loss: 1.8541368246078491 acc: 0.27925 max_grad: 5.510813\n",
      "epoch= 0 step= 240 loss: 1.7668964862823486 acc: 0.32575 max_grad: 10.063973\n",
      "epoch= 0 step= 280 loss: 1.7065242528915405 acc: 0.31875 max_grad: 15.0\n",
      "epoch= 0 step= 320 loss: 1.5661728382110596 acc: 0.32375 max_grad: 1.6568742\n",
      "epoch= 0 step= 440 loss: 1.452691912651062 acc: 0.41525 max_grad: 10.038286\n",
      "epoch= 0 step= 480 loss: 1.642988681793213 acc: 0.4055 max_grad: 10.916514\n",
      "test acc: 0.4179 test loss: 153.21775817871094\n",
      "epoch= 1 step= 0 loss: 1.478299617767334 acc: 0.42 max_grad: 12.54433\n",
      "epoch= 1 step= 40 loss: 1.4729194641113281 acc: 0.42875 max_grad: 11.174403\n",
      "epoch= 1 step= 80 loss: 1.436091423034668 acc: 0.441 max_grad: 9.899188\n",
      "epoch= 1 step= 120 loss: 1.5529203414916992 acc: 0.4575 max_grad: 5.1917276\n",
      "epoch= 2 step= 240 loss: 1.2465314865112305 acc: 0.602 max_grad: 7.1366673\n",
      "epoch= 2 step= 280 loss: 1.3397266864776611 acc: 0.61725 max_grad: 11.3386755\n",
      "epoch= 2 step= 320 loss: 1.1300482749938965 acc: 0.622 max_grad: 15.0\n",
      "epoch= 2 step= 360 loss: 1.1102454662322998 acc: 0.61975 max_grad: 7.558742\n",
      "epoch= 2 step= 400 loss: 1.2532609701156616 acc: 0.634 max_grad: 11.627354\n",
      "epoch= 2 step= 440 loss: 1.2097793817520142 acc: 0.64175 max_grad: 15.000001\n",
      "epoch= 2 step= 480 loss: 0.968299388885498 acc: 0.64475 max_grad: 7.375768\n",
      "epoch= 3 step= 200 loss: 0.9289438724517822 acc: 0.667 max_grad: 7.695727\n",
      "epoch= 3 step= 240 loss: 0.9037051200866699 acc: 0.69375 max_grad: 5.2987537\n",
      "epoch= 3 step= 280 loss: 0.8186740875244141 acc: 0.67075 max_grad: 7.3271284\n",
      "epoch= 3 step= 320 loss: 0.8781833648681641 acc: 0.67975 max_grad: 7.8719063\n",
      "epoch= 3 step= 360 loss: 0.7496026754379272 acc: 0.69375 max_grad: 7.5958095\n",
      "epoch= 3 step= 400 loss: 0.8443859219551086 acc: 0.71225 max_grad: 10.414015\n",
      "epoch= 3 step= 440 loss: 0.886533260345459 acc: 0.7 max_grad: 10.714272\n",
      "epoch= 3 step= 480 loss: 0.737229585647583 acc: 0.70925 max_grad: 12.876583\n",
      "test acc: 0.697 test loss: 87.32810974121094\n",
      "epoch= 4 step= 0 loss: 0.647564709186554 acc: 0.76 max_grad: 6.0923615\n",
      "epoch= 4 step= 40 loss: 0.6780175566673279 acc: 0.724 max_grad: 11.133704\n",
      "epoch= 4 step= 80 loss: 0.764301061630249 acc: 0.72 max_grad: 6.173271\n",
      "epoch= 4 step= 120 loss: 0.7342146039009094 acc: 0.72575 max_grad: 8.9185295\n",
      "epoch= 4 step= 160 loss: 0.5806030035018921 acc: 0.73275 max_grad: 8.250427\n",
      "epoch= 4 step= 200 loss: 0.6974966526031494 acc: 0.74125 max_grad: 6.6757474\n",
      "epoch= 4 step= 240 loss: 0.7092955708503723 acc: 0.74375 max_grad: 12.6831255\n",
      "epoch= 4 step= 280 loss: 0.7306291460990906 acc: 0.74025 max_grad: 7.3185844\n",
      "epoch= 4 step= 320 loss: 0.5827285647392273 acc: 0.74675 max_grad: 7.0877705\n",
      "epoch= 4 step= 360 loss: 0.6799349188804626 acc: 0.746 max_grad: 12.537602\n",
      "epoch= 4 step= 400 loss: 0.5529042482376099 acc: 0.76125 max_grad: 5.6044674\n",
      "epoch= 4 step= 440 loss: 0.6804747581481934 acc: 0.74625 max_grad: 8.128693\n",
      "epoch= 4 step= 480 loss: 0.8024899363517761 acc: 0.7565 max_grad: 11.881129\n",
      "test acc: 0.7022 test loss: 87.8638916015625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for step, (x, y) in enumerate(dataset_train):\n",
    "        # [b, 1] => [b]\n",
    "        y = tf.squeeze(y, axis=1)\n",
    "        # [b, 10]\n",
    "        y = tf.one_hot(y, depth=10)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss = criteon(y, logits)\n",
    "            # loss2 = compute_loss(logits, tf.argmax(y, axis=1))\n",
    "            # mse_loss = tf.reduce_sum(tf.square(y-logits))\n",
    "            # print(y.shape, logits.shape)\n",
    "            metric.update_state(y, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # MUST clip gradient here or it will disconverge!\n",
    "        grads = [ tf.clip_by_norm(g, 15) for g in grads]\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 40 == 0:\n",
    "            max_grad=-1e-6\n",
    "            for g in grads:\n",
    "                tmp_grad=tf.norm(g).numpy()\n",
    "                if tmp_grad > max_grad: max_grad=tmp_grad\n",
    "                #print(tf.norm(g).numpy())\n",
    "                \n",
    "            print(\"epoch=\", epoch, \"step=\", step, 'loss:', float(loss), 'acc:', metric.result().numpy(), \"max_grad:\", max_grad)\n",
    "            metric.reset_states()\n",
    "\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "\n",
    "        test_acc = keras.metrics.CategoricalAccuracy()\n",
    "        test_loss = 0.0\n",
    "        for x, y in dataset_test:\n",
    "            # [b, 1] => [b]\n",
    "            y = tf.squeeze(y, axis=1)\n",
    "            # [b, 10]\n",
    "            y = tf.one_hot(y, depth=10)\n",
    "            logits = model.predict(x)\n",
    "            # be careful, these functions can accept y as [b] without warnning.\n",
    "            test_acc.update_state(y, logits)\n",
    "            test_loss += criteon(y, logits)\n",
    "        print('test acc:', test_acc.result().numpy(), \"test loss:\", float(test_loss))\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
