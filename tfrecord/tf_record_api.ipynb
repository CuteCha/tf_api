{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc1\n",
      "sys.version_info(major=3, minor=6, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 2.1.0\n",
      "numpy 1.18.0\n",
      "pandas 0.20.3\n",
      "sklearn 0.21.3\n",
      "tensorflow 2.0.0-rc1\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl,np,pd,sklearn,tf,keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfrecord 文件格式\n",
    "-> tf.train.Example\n",
    "     ->tf.train.Features->{\"key\":tf.train.Feature}\n",
    "        -> tf.train.Feature -> tf.train.ByteList/FloatList/Int64List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'machine learning', b'cc150']\n",
      "value: 15.5\n",
      "value: 9.5\n",
      "value: 7.0\n",
      "value: 8.0\n",
      "\n",
      "value: 42\n",
      "\n",
      "feature {\n",
      "  key: \"age\"\n",
      "  value {\n",
      "    int64_list {\n",
      "      value: 42\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  key: \"favorite_books\"\n",
      "  value {\n",
      "    bytes_list {\n",
      "      value: \"machine learning\"\n",
      "      value: \"cc150\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  key: \"hours\"\n",
      "  value {\n",
      "    float_list {\n",
      "      value: 15.5\n",
      "      value: 9.5\n",
      "      value: 7.0\n",
      "      value: 8.0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 42\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"favorite_books\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"machine learning\"\n",
      "        value: \"cc150\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"hours\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 15.5\n",
      "        value: 9.5\n",
      "        value: 7.0\n",
      "        value: 8.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "b'\\n\\\\\\n\\x1d\\n\\x05hours\\x12\\x14\\x12\\x12\\n\\x10\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xe0@\\x00\\x00\\x00A\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01*'\n"
     ]
    }
   ],
   "source": [
    "favorite_books = [name.encode('utf-8') for name in [\"machine learning\",\"cc150\"]]\n",
    "\n",
    "favorite_books_bytelist = tf.train.BytesList(value = favorite_books)\n",
    "print(favorite_books)\n",
    "\n",
    "hours_floatlist = tf.train.FloatList(value = [15.5,9.5,7.0,8.0])\n",
    "print(hours_floatlist)\n",
    "\n",
    "age_int64list = tf.train.Int64List(value = [42])\n",
    "print(age_int64list)\n",
    "\n",
    "features = tf.train.Features(\n",
    "    feature = {\n",
    "        \"favorite_books\":tf.train.Feature(bytes_list=favorite_books_bytelist),\n",
    "        \"hours\":tf.train.Feature(float_list = hours_floatlist),\n",
    "        \"age\":tf.train.Feature(int64_list = age_int64list),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(features)\n",
    "\n",
    "example = tf.train.Example(features = features)\n",
    "print(example)\n",
    "\n",
    "serialized_example = example.SerializeToString()\n",
    "print(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = 'tfrecord_basic'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "filename = \"test.tfrecord\"\n",
    "filename_fullpath = os.path.join(output_dir,filename)\n",
    "with tf.io.TFRecordWriter(filename_fullpath) as writer:\n",
    "    for i in range(3):\n",
    "        writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\n\\\\\\n\\x1d\\n\\x05hours\\x12\\x14\\x12\\x12\\n\\x10\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xe0@\\x00\\x00\\x00A\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01*', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\n\\\\\\n\\x1d\\n\\x05hours\\x12\\x14\\x12\\x12\\n\\x10\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xe0@\\x00\\x00\\x00A\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01*', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\n\\\\\\n\\x1d\\n\\x05hours\\x12\\x14\\x12\\x12\\n\\x10\\x00\\x00xA\\x00\\x00\\x18A\\x00\\x00\\xe0@\\x00\\x00\\x00A\\n-\\n\\x0efavorite_books\\x12\\x1b\\n\\x19\\n\\x10machine learning\\n\\x05cc150\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01*', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
    "for serialized_example_tensor in dataset:\n",
    "    print(serialized_example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n",
      "cc150\n",
      "machine learning\n",
      "cc150\n",
      "machine learning\n",
      "cc150\n"
     ]
    }
   ],
   "source": [
    "expected_features = {\n",
    "    \"favorite_books\":tf.io.VarLenFeature(dtype = tf.string),\n",
    "    'hours':tf.io.VarLenFeature(dtype = tf.float32),\n",
    "    \"age\":tf.io.FixedLenFeature([],dtype = tf.int64),\n",
    "}\n",
    "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
    "for serialized_example_tensor in dataset:\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example_tensor,\n",
    "        expected_features)\n",
    "    books = tf.sparse.to_dense(example[\"favorite_books\"],default_value = b\"\")\n",
    "    for book in books:\n",
    "        print(book.numpy().decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#压缩形式存储和读取\n",
    "filename_fullpath_zip = filename_fullpath + '.zip'\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(filename_fullpath_zip,options) as writer:\n",
    "    for i in range(3):\n",
    "        writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n",
      "cc150\n",
      "machine learning\n",
      "cc150\n",
      "machine learning\n",
      "cc150\n"
     ]
    }
   ],
   "source": [
    "dataset_zip = tf.data.TFRecordDataset([filename_fullpath_zip],compression_type=\"GZIP\")\n",
    "for serialized_example_tensor in dataset_zip:\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example_tensor,\n",
    "        expected_features)\n",
    "    books = tf.sparse.to_dense(example[\"favorite_books\"],default_value = b\"\")\n",
    "    for book in books:\n",
    "        print(book.numpy().decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n",
      "[[ 8.32520000e+00  4.10000000e+01  6.98412698e+00  1.02380952e+00\n",
      "   3.22000000e+02  2.55555556e+00  3.78800000e+01 -1.22230000e+02]\n",
      " [ 8.30140000e+00  2.10000000e+01  6.23813708e+00  9.71880492e-01\n",
      "   2.40100000e+03  2.10984183e+00  3.78600000e+01 -1.22220000e+02]\n",
      " [ 7.25740000e+00  5.20000000e+01  8.28813559e+00  1.07344633e+00\n",
      "   4.96000000e+02  2.80225989e+00  3.78500000e+01 -1.22240000e+02]\n",
      " [ 5.64310000e+00  5.20000000e+01  5.81735160e+00  1.07305936e+00\n",
      "   5.58000000e+02  2.54794521e+00  3.78500000e+01 -1.22250000e+02]\n",
      " [ 3.84620000e+00  5.20000000e+01  6.28185328e+00  1.08108108e+00\n",
      "   5.65000000e+02  2.18146718e+00  3.78500000e+01 -1.22250000e+02]]\n",
      "[4.526 3.585 3.521 3.413 3.422]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)\n",
    "print(housing.data[:5,:])\n",
    "print(housing.target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(5160, 8) (5160,)\n",
      "(3870, 8) (3870,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaler = scaler.fit_transform(x_train)\n",
    "x_valid_scaler = scaler.transform(x_valid)\n",
    "x_test_scaler = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=np.c_[x_train_scaler,y_train]\n",
    "valid=np.c_[x_valid_scaler,y_valid]\n",
    "test=np.c_[x_test_scaler,y_test]\n",
    "\n",
    "np.savetxt(\"data/csv/train.csv\",train,delimiter=',')\n",
    "np.savetxt(\"data/csv/valid.csv\",valid,delimiter=',')\n",
    "np.savetxt(\"data/csv/test.csv\",test,delimiter=',')\n",
    "# print(train.shape)\n",
    "# print(train[:5,:])\n",
    "# print(x_train_scaler[:5,:])\n",
    "# print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv_line(line,n_fields=9):\n",
    "    defs = [tf.constant(np.nan)]*n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line,record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x,y\n",
    "\n",
    "# def csv_reader_dataset(filenames,n_readers=5,batch_size=32,n_parse_threads=5,\n",
    "#                       shuffle_buffer_size=10000):\n",
    "#     dataset = tf.data.Dataset.list_files(filenames)\n",
    "#     dataset = dataset.repeat()\n",
    "#     dataset = dataset.interleave(\n",
    "#         lambda filename:tf.data.TextLineDataset(filename),#tf.data.TextLineDataset(filename).skip(1)\n",
    "#         cycle_length = n_readers)\n",
    "#     dataset.shuffle(shuffle_buffer_size)\n",
    "#     dataset = dataset.map(parse_csv_line, num_parallel_calls = n_parse_threads)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "    \n",
    "#     x = tf.stack(parsed_fields[0:-1])\n",
    "#     y = tf.stack(parsed_fields[-1:])\n",
    "#     return x,y\n",
    "\n",
    "def csv_reader_dataset(filenames,n_readers=5,batch_size=32,n_parse_threads=5,\n",
    "                      shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename:tf.data.TextLineDataset(filename),\n",
    "        cycle_length = n_readers)\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line,num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=3\n",
    "train_filenames=[\"data/csv/train.csv\"]\n",
    "valid_filenames=[\"data/csv/valid.csv\"]\n",
    "test_filenames=[\"data/csv/test.csv\"]\n",
    "train_set = csv_reader_dataset(train_filenames,batch_size=batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,batch_size=batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,batch_size=batch_size)\n",
    "# for items in train_set.take(1):\n",
    "#     print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serialize_example(x,y):\n",
    "    \"\"\"Converts x,y to tf.train.Example and serialize\"\"\"\n",
    "    input_features = tf.train.FloatList(value = x)\n",
    "    label = tf.train.FloatList(value = y)\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\":tf.train.Feature(float_list = input_features),\n",
    "            \"label\":tf.train.Feature(float_list = label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()\n",
    "\n",
    "def csv_to_tfrecords(csv_filename,tfrecords_filename,compression_type = None):\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    data=np.loadtxt(csv_filename,delimiter=',')\n",
    "    \n",
    "    with tf.io.TFRecordWriter(tfrecords_filename,options) as writer:\n",
    "        for line in data:\n",
    "            x=line[:-1]\n",
    "            y=line[-1:]\n",
    "            writer.write(serialize_example(x,y))\n",
    "        \n",
    "csv_to_tfrecords(\"data/csv/train.csv\",\"data/tfrecords/train.tfrecords\")\n",
    "csv_to_tfrecords(\"data/csv/valid.csv\",\"data/tfrecords/valid.tfrecords\")\n",
    "csv_to_tfrecords(\"data/csv/test.csv\",\"data/tfrecords/test.tfrecords\")\n",
    "\n",
    "csv_to_tfrecords(\"data/csv/train.csv\",\"data/zip_tfrecords/train.tfrecords\",compression_type=\"GZIP\")\n",
    "csv_to_tfrecords(\"data/csv/valid.csv\",\"data/zip_tfrecords/valid.tfrecords\",compression_type=\"GZIP\")\n",
    "csv_to_tfrecords(\"data/csv/test.csv\",\"data/zip_tfrecords/test.tfrecords\",compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_features = {\n",
    "    \"input_features\":tf.io.FixedLenFeature([8],dtype = tf.float32),\n",
    "    \"label\":tf.io.FixedLenFeature([1],dtype = tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example, expected_features)\n",
    "    return example[\"input_features\"],example[\"label\"]\n",
    "\n",
    "def tfrecords_reader_dataset(filenames,n_readers=5,batch_size=32,n_parse_threads=5,\n",
    "                             shuffle_buffer_size=10000, compression_type = None):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename:tf.data.TFRecordDataset(\n",
    "            filename,compression_type = compression_type),\n",
    "        cycle_length = n_readers)\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_example,num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "tfr_train_filenames=[\"data/zip_tfrecords/train.tfrecords\"]\n",
    "tfr_valid_filenames=[\"data/zip_tfrecords/valid.tfrecords\"]\n",
    "tfr_test_filenames=[\"data/zip_tfrecords/test.tfrecords\"]\n",
    "\n",
    "train_dataset=tfrecords_reader_dataset(tfr_train_filenames,n_readers=1,compression_type=\"GZIP\")\n",
    "valid_dataset=tfrecords_reader_dataset(tfr_valid_filenames,n_readers=1,compression_type=\"GZIP\")\n",
    "test_dataset=tfrecords_reader_dataset(tfr_test_filenames,n_readers=1,compression_type=\"GZIP\")\n",
    "\n",
    "# for item in train_dataset.take(1):\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30,activation=\"relu\",input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5,min_delta=1e-2)]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 120 steps\n",
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 1.7591 - val_loss: 0.6028\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4647 - val_loss: 0.4610\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.4189\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3918 - val_loss: 0.3988\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3821 - val_loss: 0.4195\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3868\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.3901\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3803\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.3712\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.3678\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.3737\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3683\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3635\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3649\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    validation_data = valid_dataset,\n",
    "                    steps_per_epoch = 11160//32,\n",
    "                    validation_steps = 3870//32,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 626us/step - loss: 0.3727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37270829386962867"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset,steps = 5160//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
