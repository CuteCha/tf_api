{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc1\n",
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "import transformers\n",
    "# from transformers import *\n",
    "\n",
    "print(tf.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ab8918f6545f2b5b602348af2e2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME='hfl/chinese-bert-wwm'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.BertModel.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__',\n",
      " '__class__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattr__',\n",
      " '__getattribute__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__setstate__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_apply',\n",
      " '_backend',\n",
      " '_backward_hooks',\n",
      " '_buffers',\n",
      " '_forward_hooks',\n",
      " '_forward_pre_hooks',\n",
      " '_get_name',\n",
      " '_get_resized_embeddings',\n",
      " '_init_weights',\n",
      " '_load_from_state_dict',\n",
      " '_load_state_dict_pre_hooks',\n",
      " '_modules',\n",
      " '_named_members',\n",
      " '_parameters',\n",
      " '_prune_heads',\n",
      " '_register_load_state_dict_pre_hook',\n",
      " '_register_state_dict_hook',\n",
      " '_resize_token_embeddings',\n",
      " '_slow_forward',\n",
      " '_state_dict_hooks',\n",
      " '_tie_or_clone_weights',\n",
      " '_tracing_name',\n",
      " '_version',\n",
      " 'add_module',\n",
      " 'apply',\n",
      " 'base_model',\n",
      " 'base_model_prefix',\n",
      " 'buffers',\n",
      " 'children',\n",
      " 'config',\n",
      " 'config_class',\n",
      " 'cpu',\n",
      " 'cuda',\n",
      " 'double',\n",
      " 'dump_patches',\n",
      " 'embeddings',\n",
      " 'encoder',\n",
      " 'eval',\n",
      " 'extra_repr',\n",
      " 'float',\n",
      " 'forward',\n",
      " 'from_pretrained',\n",
      " 'get_input_embeddings',\n",
      " 'get_output_embeddings',\n",
      " 'half',\n",
      " 'init_weights',\n",
      " 'load_state_dict',\n",
      " 'load_tf_weights',\n",
      " 'modules',\n",
      " 'named_buffers',\n",
      " 'named_children',\n",
      " 'named_modules',\n",
      " 'named_parameters',\n",
      " 'parameters',\n",
      " 'pooler',\n",
      " 'pretrained_model_archive_map',\n",
      " 'prune_heads',\n",
      " 'register_backward_hook',\n",
      " 'register_buffer',\n",
      " 'register_forward_hook',\n",
      " 'register_forward_pre_hook',\n",
      " 'register_parameter',\n",
      " 'resize_token_embeddings',\n",
      " 'save_pretrained',\n",
      " 'set_input_embeddings',\n",
      " 'share_memory',\n",
      " 'state_dict',\n",
      " 'tie_weights',\n",
      " 'to',\n",
      " 'train',\n",
      " 'training',\n",
      " 'type',\n",
      " 'zero_grad']\n",
      "<class 'transformers.modeling_bert.BertModel'>\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(dir(model))\n",
    "pprint.pprint(type(model))\n",
    "pprint.pprint(type(model.get_input_embeddings()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset = valid_dataset.batch(64)\n",
    "print(type(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "                    validation_data=valid_dataset, validation_steps=7)\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./save/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  8554, 10018,  9947, 11485, 10500, 11598,  8165,  8663, 10163,\n",
      "         11707, 10334,   119,   102, 10163, 11707, 10334,  8997,  8358,  8134,\n",
      "          8797,  9007,  8862,  8663,  8554, 10018,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sentence_0 = \"This research was consistent with his findings.\"\n",
    "\n",
    "sentence_1 = \"His findings were compatible with this research.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "print(inputs_1)\n",
    "\n",
    "# pred_1 = pytorch_model(*inputs_1)[0].argmax().item()\n",
    "# pred_2 = pytorch_model(*inputs_2)[0].argmax().item()\n",
    "\n",
    "# print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "# print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
